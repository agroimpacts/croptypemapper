{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from random import shuffle\n",
    "import gc\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "import urllib.parse as urlparse\n",
    "import boto3\n",
    "import shutil\n",
    "import tqdm\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "from numpy import inf\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import rasterio\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reproducible(seed = 42):\n",
    "    \"\"\"Make all the randomization processes start from a shared seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "make_reproducible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Step 1. Rename the data such that the gridIDs in the filenames have leading zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "\"\"\"\n",
    "\n",
    "def rename_w_leading_0s(root_dir, tif_content, num_digits, ftype, country, source, verbose = False):\n",
    "    \"\"\"\n",
    "    Renames grid IDs with leading 0's so that sorting of filenames is in ascending order\n",
    "    \n",
    "    root_dir (str) -- path to the main directory which data resides. Example: \"C:/My_documents/Data\".\n",
    "    tif_content (str) -- Seperate between Labels (mask) and img data (data).\n",
    "    num_digits (int) -- Decide on the number of digits to represent a Grid-ID. Default id 6.\n",
    "    ftype (str) -- identifies format of the file to be renamed.\n",
    "    country (str) -- This is based on the organization of dataset that images and \n",
    "                     labels reside inside a country folder.\n",
    "    source (str) -- folder name of the resource. It can be either the remote sensing sensor used to\n",
    "                    acquire the image dataset or the label dataset.\n",
    "    verbose (Binary) -- If set to True, prints a report of old and new names on screen. Default is False.\n",
    "    \"\"\"\n",
    "    assert tif_content in [\"mask\", \"data\"]\n",
    "    assert country in [\"SouthSudan\", \"Ghana\"]\n",
    "    assert source in [\"Sentinel-1\", \"Sentinel-2\", \"Labels\"]\n",
    "    \n",
    "    path_to_src = Path(root_dir) / country / source\n",
    "    old_fname = []\n",
    "    new_fname = []\n",
    "    \n",
    "    if tif_content == \"mask\":\n",
    "        for dirname in os.listdir(path_to_src):\n",
    "            gridID = str(dirname).split(\"_\")[-1]\n",
    "            \n",
    "            for filename in os.listdir(path_to_src / dirname):\n",
    "                \n",
    "                if ftype == \"tif\":\n",
    "                    \n",
    "                    if filename.endswith(\".tif\"):\n",
    "                        old_fname += [path_to_src / dirname / filename]\n",
    "                        new_name = filename.replace(\".tif\", \"_\" + gridID).zfill(num_digits) + \".tif\"\n",
    "                        new_fname += [path_to_src / dirname / new_name]\n",
    "                \n",
    "                elif ftype == \"npy\":\n",
    "                    \n",
    "                    if filename.endswith(\".npy\"):\n",
    "                        old_fname += [path_to_src / dirname / filename]\n",
    "                        new_name = filename.replace(\".npy\", \"_\" + gridID).zfill(num_digits) + \".npy\"\n",
    "                        new_fname += [path_to_src / dirname / new_name]\n",
    "                    \n",
    "                    elif filename.endswith(\".json\"):\n",
    "                        old_fname += [path_to_src / dirname / filename]\n",
    "                        new_name = filename.replace(\".json\", \"_\" + gridID).zfill(num_digits) + \".json\"\n",
    "                        new_fname += [path_to_src / dirname / new_name]\n",
    "    \n",
    "    elif tif_content == 'data':\n",
    "        \n",
    "        for dirname in os.listdir(path_to_src):\n",
    "            string_list = str(dirname).split(\"_\")[-5:]\n",
    "            string_list[1] = string_list[1].zfill(num_digits)\n",
    "            replace_string = '_'.join(string_list)\n",
    "            \n",
    "            for filename in os.listdir(path_to_src / dirname):\n",
    "                \n",
    "                if filename.endswith(\".tif\"):\n",
    "                    old_fname += [path_to_src / dirname / filename]\n",
    "                    new_name = filename.replace(\".tif\", \"_\" + replace_string) + \".tif\"\n",
    "                    new_fname += [path_to_src / dirname / new_name]\n",
    "                \n",
    "                elif filename.endswith(\".json\"):\n",
    "                    old_fname += [path_to_src / dirname / filename]\n",
    "                    new_name = filename.replace(\".json\", \"_\" + replace_string) + \".json\"\n",
    "                    new_fname += [path_to_src / dirname / new_name]\n",
    "    \n",
    "    for i,j in zip(old_fname, new_fname):\n",
    "        os.rename(i, j)\n",
    "        if verbose:\n",
    "            print('Renaming {} to {}'.format(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana\"\n",
    "tif_content = \"mask\"\n",
    "num_digits = 6\n",
    "ftype = \"tif\"\n",
    "country = \"Ghana\"\n",
    "source = \"Labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rename_w_leading_0s(root_dir, tif_content, num_digits, ftype, country, source, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 2. Remove those Grid-IDs where there is no actual label recorded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import rasterio\n",
    "from collections import Counter\n",
    "import json\n",
    "\"\"\"\n",
    "\n",
    "def get_grid_nums(root_dir, country, source, ftype, verbose = False):\n",
    "    \n",
    "    src_path = Path(root_dir) / country / source\n",
    "    \n",
    "    if ftype == \"tif\":\n",
    "        files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for \\\n",
    "                 f in filenames if f.endswith(\".tif\")]\n",
    "        files.sort()\n",
    "        \n",
    "        if country == \"Ghana\":\n",
    "            grid_numbers = [str(f).split(\"_\")[-4] for f in files]\n",
    "        \n",
    "        elif country == \"SouthSudan\":\n",
    "            grid_numbers = [str(f).split(\"_\")[-3] for f in files]\n",
    "    \n",
    "    elif ftype == \"npy\":\n",
    "        files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for \\\n",
    "                 f in filenames if f.endswith(\".npy\") or f.endswith(\".json\")]\n",
    "        files.sort()\n",
    "        grid_numbers = [str(f).split(\"_\")[-4] for f in files]\n",
    "\n",
    "    grid_numbers.sort()\n",
    "    \n",
    "    if verbose:\n",
    "        for i,j in zip(grid_numbers, files):\n",
    "            print('Grid-ID: {}\\nAssociated file with same ID {}\\n'.format(i, j))\n",
    "    \n",
    "    return grid_numbers, files\n",
    "\n",
    "##################################################\n",
    "\n",
    "def get_empty_grids(root_dir, country, source, lbl_fldrname, verbose = True):\n",
    "    \"\"\"\n",
    "    Provides data from input .tif files depending on function input parameters. \n",
    "    \n",
    "    Args:\n",
    "      directory - (str) the base directory of data\n",
    "      countries - (list of str) list of strings that point to the directory names\n",
    "                  of the different countries (i.e. ['ghana', 'tanzania', 'southsudan'])\n",
    "      sources - (list of str) list of directory of satellite sources (i.e. 's1_64x64', 's2') \n",
    "      verbose - (boolean) prints outputs from function\n",
    "      ext - (str) file type that you are working with (i.e. 'tif', 'npy') \n",
    "   \n",
    "      lbl_dir - (str) the directory name that the raster labels are stored in \n",
    "                      (i.e. 'raster', 'raster_64x64')\n",
    "    \"\"\"\n",
    "\n",
    "    valid_pixels_list = []\n",
    "    empty_masks = []\n",
    "    \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "\n",
    "    mask_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for \\\n",
    "                   f in filenames if f.endswith('.tif')]\n",
    "    mask_ids = [str(f).split('_')[-1].replace('.tif', '') for f in mask_fnames]\n",
    "\n",
    "    mask_fnames.sort()\n",
    "    mask_ids.sort()\n",
    "    \n",
    "    assert len(mask_fnames) == len(mask_ids)\n",
    "\n",
    "    for mask_fname, mask_id in zip(mask_fnames, mask_ids):\n",
    "        with rasterio.open(mask_fname) as src:\n",
    "            cur_mask = src.read()\n",
    "            valid_pixels = np.sum(cur_mask > 0) \n",
    "            valid_pixels_list.append((mask_id, valid_pixels))\n",
    "            if valid_pixels == 0:\n",
    "                empty_masks.append(mask_id)\n",
    "\n",
    "    delete_me = []\n",
    "    \n",
    "    grid_numbers, source_files = get_grid_nums(root_dir, country, source, ftype, verbose = False)\n",
    "\n",
    "    all_ids = set(empty_masks + grid_numbers)\n",
    "    for el in all_ids:\n",
    "        if el in empty_masks and el in grid_numbers:\n",
    "            delete_me.append(el)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"valid pixels list: \", len(valid_pixels_list))\n",
    "        print('empty masks: ', len(empty_masks))\n",
    "        print('delete me length: ', len(delete_me))\n",
    "        print('delete me: ', delete_me)\n",
    "        \n",
    "    return set(delete_me)\n",
    "\n",
    "##################################################\n",
    "\n",
    "def remove_irrelevant_files(root_dir, country, source, delete_list, ftype, verbose = True):\n",
    "    \n",
    "    if len(delete_list) == 0:\n",
    "        print(\"There is no empty grid to remove.\")\n",
    "    \n",
    "    else:\n",
    "        grid_nums, source_files = get_grid_nums(root_dir, country, source, ftype, verbose = False)\n",
    "    \n",
    "        for grid_to_rm in delete_list:\n",
    "            files_to_rm = [str(f) for f in source_files if ''.join(['_', grid_to_rm]) in str(f)]\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"grid to remove: {}\\n\".format(grid_to_rm))\n",
    "            print(\"files to remove: {}\\n\".format(files_to_rm))\n",
    "        \n",
    "        #Remove files        \n",
    "        [os.remove(f) for f in files_to_rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana\"\n",
    "lbl_fldrname = \"Labels\"\n",
    "ftype = \"tif\"\n",
    "country = \"Ghana\"\n",
    "source = \"Sentinel-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grids_to_delete = get_empty_grids(root_dir, country, source, lbl_fldrname)\n",
    "\n",
    "remove_irrelevant_files(root_dir, country, source, grids_to_delete, ftype, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 3. Get image statistics for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_img_stats(root_dir, country, source, clip):\n",
    "\n",
    "    #src_path = Path(\"C:/My_documents/CropTypeData_Rustowicz/example_tile539059/sentinel2_time_series_best\")\n",
    "    src_path = Path(root_dir).joinpath(country , source)\n",
    "    files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".tif\") if \"source\" in f]\n",
    "    \n",
    "    tile_min =[]\n",
    "    tile_max =[]\n",
    "\n",
    "    for file in tqdm.tqdm(files):\n",
    "        with rasterio.open(file, \"r\") as src:\n",
    "            tile = src.read()\n",
    "            tile = tile.astype(float)\n",
    "        \n",
    "            nan_Corr_img = np.where(tile == 0, np.nan, tile)\n",
    "            \n",
    "            for i in range(tile.shape[0]):\n",
    "                \n",
    "                if np.isnan(nan_Corr_img[i, :, :]).all():\n",
    "                    band_min = 10e9\n",
    "                    tile_min.append(band_min)        \n",
    "                    band_max = -10e9\n",
    "                    tile_max.append(band_max)\n",
    "                else:\n",
    "                    left_tail_clip = np.nanpercentile(nan_Corr_img[i, :, :], clip)\n",
    "                    right_tail_clip = np.nanpercentile(nan_Corr_img[i, :, :], 100 - clip)\n",
    "                    left_clipped_band = np.where(nan_Corr_img[i, :, :] < left_tail_clip, left_tail_clip, nan_Corr_img[i, :, :])\n",
    "                    clipped_band = np.where(left_clipped_band > right_tail_clip, right_tail_clip, left_clipped_band)\n",
    "                \n",
    "                    band_min = np.nanmin(clipped_band)\n",
    "                    tile_min.append(band_min)        \n",
    "                    band_max = np.nanmax(clipped_band)\n",
    "                    tile_max.append(band_max)\n",
    "\n",
    "    if source == \"Sentinel-1\":\n",
    "        b1_min =10e9; b2_min =10e9; b3_min =10e9\n",
    "        b1_max =-10e9; b2_max =-10e9; b3_max =-10e9\n",
    "        \n",
    "        # re-arrange the list into tuples of 3 elements to consider each band separately.\n",
    "        tile_min_rearr = [tile_min[i:i+3] for i in range(0, len(tile_min), 3)]\n",
    "        tile_max_rearr = [tile_max[i:i+3] for i in range(0, len(tile_max), 3)]\n",
    "        \n",
    "        assert len(tile_min_rearr) == len(tile_max_rearr)\n",
    "        \n",
    "        for i in range(len(tile_min_rearr)):\n",
    "            \n",
    "            if tile_min_rearr[i][0] < b1_min:\n",
    "                b1_min = tile_min_rearr[i][0]\n",
    "            if tile_min_rearr[i][1] < b2_min:\n",
    "                b2_min = tile_min_rearr[i][1]\n",
    "            if tile_min_rearr[i][2] < b3_min:\n",
    "                b3_min = tile_min_rearr[i][2]\n",
    "            \n",
    "            if tile_max_rearr[i][0] > b1_max:\n",
    "                b1_max = tile_max_rearr[i][0]\n",
    "            if tile_max_rearr[i][1] > b2_max:\n",
    "                b2_max = tile_max_rearr[i][1]\n",
    "            if tile_max_rearr[i][2] > b3_max:\n",
    "                b3_max = tile_max_rearr[i][2]\n",
    "        \n",
    "        print(\"----- Sentinel-1 range statistics per band -----\")\n",
    "        print(\"B1 ('VV') --> min:{}, max:{}\".format(b1_min, b1_max))\n",
    "        print(\"B2 ('VH') --> min:{}, max:{}\".format(b2_min, b2_max))\n",
    "        print(\"B3 ('VH/VV) --> min:{}, max:{}\".format(b3_min, b3_max))\n",
    "        \n",
    "        return [(b1_min, b2_min, b3_min),\n",
    "                (b1_max, b2_max, b3_max)]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        b1_min = 10e9; b2_min = 10e9; b3_min = 10e9; b4_min = 10e9; b5_min = 10e9; \n",
    "        b6_min = 10e9; b7_min = 10e9; b8_min = 10e9; b9_min = 10e9; b10_min = 10e9;\n",
    "        b1_max = 10e-9; b2_max = 10e-9; b3_max = 10e-9; b4_max = 10e-9; b5_max = 10e-9;\n",
    "        b6_max = 10e-9; b7_max = 10e-9; b8_max = 10e-9; b9_max = 10e-9; b10_max = 10e-9;\n",
    "        \n",
    "        tile_min_rearr = [tile_min[i:i+10] for i in range(0, len(tile_min), 10)]\n",
    "        tile_max_rearr = [tile_max[i:i+10] for i in range(0, len(tile_max), 10)]\n",
    "        \n",
    "        assert len(tile_min_rearr) == len(tile_max_rearr)\n",
    "        \n",
    "        for i in range(len(tile_min_rearr)):\n",
    "            if tile_min_rearr[i][0] < b1_min:\n",
    "                b1_min = tile_min_rearr[i][0]\n",
    "            if tile_min_rearr[i][1] < b2_min:\n",
    "                b2_min = tile_min_rearr[i][1]\n",
    "            if tile_min_rearr[i][2] < b3_min:\n",
    "                b3_min = tile_min_rearr[i][2]\n",
    "            if tile_min_rearr[i][3] < b4_min:\n",
    "                b4_min = tile_min_rearr[i][3]\n",
    "            if tile_min_rearr[i][4] < b5_min:\n",
    "                b5_min = tile_min_rearr[i][4]\n",
    "            if tile_min_rearr[i][5] < b6_min:\n",
    "                b6_min = tile_min_rearr[i][5]\n",
    "            if tile_min_rearr[i][6] < b7_min:\n",
    "                b7_min = tile_min_rearr[i][6]\n",
    "            if tile_min_rearr[i][7] < b8_min:\n",
    "                b8_min = tile_min_rearr[i][7]\n",
    "            if tile_min_rearr[i][8] < b9_min:\n",
    "                b9_min = tile_min_rearr[i][8]\n",
    "            if tile_min_rearr[i][9] < b10_min:\n",
    "                b10_min = tile_min_rearr[i][9]\n",
    "            \n",
    "            if tile_max_rearr[i][0] > b1_max:\n",
    "                b1_max = tile_max_rearr[i][0]\n",
    "            if tile_max_rearr[i][1] > b2_max:\n",
    "                b2_max = tile_max_rearr[i][1]\n",
    "            if tile_max_rearr[i][2] > b3_max:\n",
    "                b3_max = tile_max_rearr[i][2]\n",
    "            if tile_max_rearr[i][3] > b4_max:\n",
    "                b4_max = tile_max_rearr[i][3]\n",
    "            if tile_max_rearr[i][4] > b5_max:\n",
    "                b5_max = tile_max_rearr[i][4]\n",
    "            if tile_max_rearr[i][5] > b6_max:\n",
    "                b6_max = tile_max_rearr[i][5]\n",
    "            if tile_max_rearr[i][6] > b7_max:\n",
    "                b7_max = tile_max_rearr[i][6]\n",
    "            if tile_max_rearr[i][7] > b8_max:\n",
    "                b8_max = tile_max_rearr[i][7]\n",
    "            if tile_max_rearr[i][8] > b9_max:\n",
    "                b9_max = tile_max_rearr[i][8]\n",
    "            if tile_max_rearr[i][9] > b10_max:\n",
    "                b10_max = tile_max_rearr[i][9]\n",
    "\n",
    "        print(\"----- Sentinel-2 range statistics per band -----\")\n",
    "        print(\"B1 ('Blue') --> min:{}, max:{}\".format(b1_min, b1_max))\n",
    "        print(\"B2 ('Green') --> min:{}, max:{}\".format(b2_min, b2_max))\n",
    "        print(\"B3 ('Red') --> min:{}, max:{}\".format(b3_min, b3_max))\n",
    "        print(\"B4 ('Red Edge 1') --> min:{}, max:{}\".format(b4_min, b4_max))\n",
    "        print(\"B5 ('Red Edge 2') --> min:{}, max:{}\".format(b5_min, b5_max))\n",
    "        print(\"B6 ('Red Edge 3') --> min:{}, max:{}\".format(b6_min, b6_max))\n",
    "        print(\"B7 ('NIR') --> min:{}, max:{}\".format(b7_min, b7_max))\n",
    "        print(\"B8 ('Red Edge 4') --> min:{}, max:{}\".format(b8_min, b8_max))\n",
    "        print(\"B9 ('SWIR 1') --> min:{}, max:{}\".format(b9_min, b9_max))\n",
    "        print(\"B10 ('SWIR 2') --> min:{}, max:{}\".format(b10_min, b10_max))\n",
    "        \n",
    "        return [(b1_min, b2_min, b3_min, b4_min, b5_min, b6_min, b7_min, b8_min, b9_min, b10_min), \n",
    "                (b1_max, b2_max, b3_max, b4_max, b5_max, b6_max, b7_max, b8_max, b9_max, b10_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#root_dir = \"C:/My_documents/CropTypeData_Rustowicz/example_tile539059/sentinel2_time_series_best\"\n",
    "#root_dir = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana\"\n",
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "source = \"Sentinel-1\"\n",
    "clip = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_img_stats(root_dir, country, source, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 4. Normalize tiles, add doy band to both sources, removes tiles with bands of NaN, add spectral indices to sentinel-2 (optional) and create temporal stacks for each grid and save them as .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "\n",
    "def normalize(grid, source, country, norm_type, clip):\n",
    "    r\"\"\" Normalization based on the chosen normalization type.\n",
    "    Args: \n",
    "      grid (numpy array) -- grid to be normalized.\n",
    "      norm_type (str) -- decide on the type of normalization. either z-value (standardization) or min/max.\n",
    "      source (str) -- Satellite sensor.\n",
    "      country (str) -- Geographic region where the dataset is taken.\n",
    "    \n",
    "    Returns:\n",
    "      grid (numpy array) -- a normalized version of the input grid.\n",
    "      \n",
    "    NOTE: Both normalizations are based on the statistics of the whole temporal and spatial extent for each band.\n",
    "    \"\"\"\n",
    "    \n",
    "    # hard-coded global statistics over the spatial and temporal extent required for normalization. \n",
    "    MEANS = {\n",
    "        \"Sentinel-1\": {\"Ghana\": np.array([-10.50, -17.24, 1.17])},\n",
    "        \"Sentinel-2\": {\"Ghana\": np.array([2620.00, 2519.89, 2630.31, 2739.81, 3225.22,\n",
    "                                          3562.64, 3356.57, 3788.05, 2915.40, 2102.65])}\n",
    "    }\n",
    "\n",
    "\n",
    "    STDS = {\n",
    "        \"Sentinel-1\": {\"Ghana\": np.array([3.57, 4.86, 5.60])},     \n",
    "        \"Sentinel-2\": {\"Ghana\": np.array([2171.62, 2085.69, 2174.37, 2084.56, 2058.97,\n",
    "                                          2117.31, 1988.70, 2099.78, 1209.48, 918.19])}\n",
    "    }\n",
    "    \n",
    "    MINS = {\n",
    "        \"Sentinel-1\": {\"Ghana\": np.array([-24.2179511259, -29.877275167, -22.03031768])},\n",
    "        \"Sentinel-2\": {\"Ghana\": np.array([735.0, 578.0, 385.425, 426.0, 490.4245, \n",
    "                                          528.0, 434.0, 456.0, 174.0, 84.0])}\n",
    "    }\n",
    "\n",
    "\n",
    "    MAXS = {\n",
    "        \"Sentinel-1\": {\"Ghana\": np.array([1.353708618, -0.30558315, 29.4155171438])},\n",
    "        \"Sentinel-2\": {\"Ghana\": np.array([14077.45, 13880.575, 14869.3, 14414.575, 14802.3, \n",
    "                                          15427.15, 14417.0, 15758.575, 14392.0, 14211.025])}\n",
    "    }\n",
    "    \n",
    "    num_bands = grid.shape[0]\n",
    "    \n",
    "    if norm_type == \"z-value\":\n",
    "        \n",
    "        means = MEANS[source][country]\n",
    "        stds = STDS[source][country]\n",
    "        \n",
    "        for i in range(num_bands):\n",
    "            grid[i,:,:] = (grid[i,:,:] - means[i]) / stds[i]\n",
    "        \n",
    "        return grid\n",
    "    \n",
    "    elif norm_type == \"min/max\":\n",
    "        \n",
    "        mins = MINS[source][country]\n",
    "        maxs = MAXS[source][country]\n",
    "        \n",
    "        if clip:\n",
    "            normalized_bands = []\n",
    "            \n",
    "            for i in range(num_bands):\n",
    "                \n",
    "                nan_corr_img = np.where(grid[i, :, :] == 0, np.nan, grid[i, :, :])\n",
    "                \n",
    "                left_tail_clip = np.nanpercentile(nan_corr_img, clip)\n",
    "                right_tail_clip = np.nanpercentile(nan_corr_img, 100 - clip)\n",
    "                \n",
    "                left_clipped_band = np.where(nan_corr_img < left_tail_clip, left_tail_clip, grid[i, :, :])\n",
    "                clipped_band = np.where(left_clipped_band > right_tail_clip, right_tail_clip, left_clipped_band)\n",
    "                \n",
    "                normalized_band = (clipped_band - mins[i]) / (maxs[i] - mins[i])\n",
    "                normalized_bands.append(np.expand_dims(normalized_band, 0))\n",
    "            \n",
    "            normal_grid = np.concatenate(normalized_bands, 0)\n",
    "            normal_grid = np.where(grid == 0, 0, normal_grid)\n",
    "            \n",
    "            return normal_grid\n",
    "        \n",
    "        else:\n",
    "            for i in range(num_bands):\n",
    "                grid[i,:,:] = (grid[i,:,:]  - mins[i]) / (maxs[i] - mins[i])\n",
    "\n",
    "    return grid\n",
    "\n",
    "#########################\n",
    "\n",
    "def date2doy(date, in_shape, norm_type, doy_mode, origin):\n",
    "    r\"\"\"\n",
    "    Convert string dates to equivalent day of the year and convert it to a Z-norm.\n",
    "\n",
    "    Parameters:\n",
    "        date (string) -- list of dates read from a .json file.\n",
    "        in_shape (tuple ) -- Day of the year will be broadcast to the specified shape.\n",
    "        norm_type (str) -- normalization method. Better match with the normalization\n",
    "                           type of the other bands.\n",
    "        doy_mode (str) -- If absolute the start of the year will be Jan 1st. For relative mode,\n",
    "                          the start_date (first timestamp) will be used as point of origin.\n",
    "        origin (str [format yyyy_mm_dd]) -- start date used with relative doy_mode.\n",
    "\n",
    "    Returns:\n",
    "        grid (np.array): Z-norm day of the year band in specified shape.\n",
    "\n",
    "    NOTE: The code assumes a 365 days year. If the length of 'relative' mode is different then\n",
    "          the normalization must revised accordingly.\n",
    "    \"\"\"\n",
    "    if doy_mode == \"absolute\":\n",
    "        date = datetime.strptime(date, '%Y_%m_%d').date()\n",
    "        doy = date.timetuple().tm_yday\n",
    "        doy = np.array([doy])\n",
    "    else:\n",
    "        date_0 = datetime.strptime(origin, '%Y_%m_%d').date()\n",
    "        date = datetime.strptime(date, '%Y_%m_%d').date()\n",
    "        # add 1 to shift the origin\n",
    "        doy = (date - date_0).days + 1\n",
    "        doy = np.array([doy])\n",
    "\n",
    "    if norm_type == \"z-value\":\n",
    "        norm_doy = (doy - 177.5) / 177.5\n",
    "    else:\n",
    "        norm_doy = (doy - 1) / 364.0\n",
    "\n",
    "    C, W, H = in_shape\n",
    "\n",
    "    stack = norm_doy[np.newaxis, :]\n",
    "    stack = np.broadcast_to(stack, (W, H))\n",
    "    stack = stack[np.newaxis, :]\n",
    "\n",
    "    return stack\n",
    "\n",
    "#########################\n",
    "\n",
    "def get_spectral_indices(img):\n",
    "\n",
    "    assert img.shape[0], \"Incorrect number of bands.\"\n",
    "\n",
    "    # Get bands with true names\n",
    "    S2_BANDS = {\"BLUE\": 0, \"GREEN\": 1, \"RED\": 2, \"RDED1\": 3, \"RDED2\": 4,\n",
    "                \"RDED3\": 5, \"NIR\": 6, \"RDED4\": 7, \"SWIR1\": 8, \"SWIR2\": 9}\n",
    "    blue = img[S2_BANDS[\"BLUE\"], :, :]\n",
    "    green = img[S2_BANDS[\"GREEN\"], :, :]\n",
    "    red = img[S2_BANDS[\"RED\"], :, :]\n",
    "    nir = img[S2_BANDS[\"NIR\"], :, :]\n",
    "    swir1 = img[S2_BANDS[\"SWIR1\"], :, :]\n",
    "    swir2 = img[S2_BANDS[\"SWIR2\"], :, :]\n",
    "\n",
    "    G = 2.5\n",
    "    C1 = 6\n",
    "    C2 = 7.5\n",
    "    L = 0.5\n",
    "    \n",
    "    ndvi = (nir - red) / (nir + red)  # Normalized Difference Vegetation Index\n",
    "    evi = G * (nir - red) / (nir + C1 * red - C2 * blue + L)  # Enhanced Vegetation Index\n",
    "    ndwi = (nir - swir2) / (nir + swir2)  # Normalized Difference Water Index\n",
    "    bi = np.sqrt(((red * red) / (green * green)) / 2)\n",
    "    \n",
    "    \"\"\"\n",
    "    Absorption properties of the middle infrared band cause a low reflectance of rice plants in this\n",
    "    channel (Lilliesand & Kiefer 1994). In irrigated rice fields, especially in early transplanting \n",
    "    periods, water environment plays an important role in rice spectral (Nuarsa et al., 2011). \n",
    "    Rice Growth Vegetation Index\n",
    "    \"\"\"\n",
    "    rgvi = 1 - (blue + red) / (nir + swir1 + swir2)\n",
    "\n",
    "    stack = np.dstack([ndvi, evi, ndwi, rgvi, bi]).transpose(2, 0, 1)\n",
    "\n",
    "    return stack\n",
    " \n",
    "##################################################\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "import rasterio\n",
    "from collections import Counter\n",
    "import json\n",
    "\"\"\"\n",
    "\n",
    "def make_temporal_cube(root_dir, country, source, lbl_fldrname, out_path=None, norm_type=\"min/max\", clip=None, \n",
    "                       channel_first=True, add_doy=True, doy_mode=\"absolute\", origin=None, add_si=False, verbose=False):\n",
    "    \n",
    "    #set_trace()\n",
    "    # Path to \"Label\" folder configuration \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
    "    lbl_ids = [str(f).split(\"_\")[-1].replace(\".tif\", \"\") for f in lbl_fnames]\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Number of grids for country {}: {}\".format(country, len(lbl_ids)))\n",
    "    \n",
    "    # Path to \"source\" folder which contains img tiles.\n",
    "    src_path = Path(root_dir) / country / source\n",
    "    \n",
    "    if out_path is None:\n",
    "        out_path = src_path.joinpath(\"npy\")\n",
    "    Path(out_path).mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "    # List of img tiles for the current source (RS sensor) and get the Grid-ID of each img tile..\n",
    "    src_files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".tif\") if \"source\" in f]\n",
    "    grid_numbers = [str(f).split(\"_\")[-4] for f in src_files]\n",
    "    \n",
    "    src_files.sort()\n",
    "    grid_numbers.sort()\n",
    "    \n",
    "    if source == \"Sentinel-2\":\n",
    "        # cloud_mask categories --> {\"clear\":0, \"cloud\":1, \"haze\":2, \"shadow\":3}\n",
    "        cloud_masks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".tif\") if \"cloudmask\" in f]\n",
    "        cloud_masks.sort()\n",
    "    \n",
    "    # read one image from list to get dimensions\n",
    "    with rasterio.open(src_files[0]) as src:\n",
    "        meta = src.meta\n",
    "        img = src.read()\n",
    "    \n",
    "    bands = meta[\"count\"]\n",
    "    if add_doy:\n",
    "        bands = meta[\"count\"] + 1\n",
    "    if (source == \"Sentinel-2\") and add_si:\n",
    "        bands = bands + 5\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-----------------------------\")\n",
    "        print(\"Image dimensions: {}\".format(img.shape))\n",
    "        print(\"Current data source: {}\".format(source))\n",
    "        print(\"Number of grids in set: {}\".format(len(set(grid_numbers))))\n",
    "        print(\"Maximum timestamps from this data source: {}\".format(Counter(grid_numbers).most_common(1)[0][1]))\n",
    "        print(\"Set of grid numbers: {}\".format(sorted(set(grid_numbers))))\n",
    "              \n",
    "    for grid_idx, grid in enumerate(sorted(set(grid_numbers))):\n",
    "        if verbose:\n",
    "            print(\"Grid: {}\".format(grid))\n",
    "        \n",
    "        cur_grid_files = [str(f) for f in src_files if \"_\" + grid + \"_\" in str(f)]\n",
    "        cur_grid_files.sort()\n",
    "        \n",
    "        usable_cur_grid_files = []\n",
    "        dates = []\n",
    "        \n",
    "        for cur_fn in cur_grid_files:\n",
    "            with rasterio.open(cur_fn) as src:\n",
    "                src_array = src.read()\n",
    "            num_nans = np.count_nonzero(np.isnan(src_array))\n",
    "            #if not num_nans >= (src_array.shape[1] * src_array.shape[1]):\n",
    "            if not num_nans > 0:\n",
    "                src_date_parts = Path(cur_fn).name.replace(\".tif\", \"\").split(\"_\")[-3:]\n",
    "                date = \"_\".join(src_date_parts)\n",
    "                dates.append(date)\n",
    "                usable_cur_grid_files.append(cur_fn)\n",
    "        usable_cur_grid_files.sort()\n",
    "        \n",
    "        if verbose:\n",
    "            diff = len(cur_grid_files) - len(usable_cur_grid_files)\n",
    "            total = len(cur_grid_files)\n",
    "            print(f\"Droping {diff} bad timestamps out of {total}.\")\n",
    "                \n",
    "        if (doy_mode == \"relative\") and not origin:\n",
    "            origin_date_parts = Path(usable_cur_grid_files[0]).name.replace(\".tif\", \"\").split(\"_\")[-3:]\n",
    "            origin = \"_\".join(origin_date_parts)\n",
    "        \n",
    "        if source == \"Sentinel-2\":\n",
    "            cur_mask_files = [str(f) for f in cloud_masks if \"_\" + grid + \"_\" in str(f)]\n",
    "            cur_mask_files.sort()\n",
    "            \n",
    "            usable_cur_mask_files = []\n",
    "            if len(cur_mask_files) != len(usable_cur_grid_files):\n",
    "                for fn in cur_mask_files:\n",
    "                    mask_date_parts = Path(fn).name.replace(\".tif\", \"\").split(\"_\")[-3:]\n",
    "                    date = \"_\".join(mask_date_parts)\n",
    "                    if date in dates:\n",
    "                        usable_cur_mask_files.append(fn)\n",
    "            else:\n",
    "                usable_cur_mask_files = cur_mask_files\n",
    "            \n",
    "            assert len(usable_cur_mask_files) == len(usable_cur_grid_files)\n",
    "            \n",
    "            # dimensions: bands x rows x columns x timestamps\n",
    "            data_array = np.zeros((bands, img.shape[1], img.shape[2], len(usable_cur_grid_files)))\n",
    "            mask_array = np.zeros((img.shape[1], img.shape[2], len(usable_cur_mask_files)))\n",
    "\n",
    "            for idx, (fname, mname) in enumerate(zip(usable_cur_grid_files, usable_cur_mask_files)):\n",
    "                if verbose:\n",
    "                    print(\"idx: \", idx)\n",
    "                    print(\"fname: \", fname)\n",
    "                    print(\"mname: \", mname)\n",
    "        \n",
    "                with rasterio.open(fname) as src:\n",
    "                    s2_tile = src.read()\n",
    "                    s2_tile = s2_tile.astype(float)\n",
    "                    if not channel_first:\n",
    "                        s2_tile.transpose(2, 0, 1)\n",
    "                        \n",
    "                    if add_si:\n",
    "                        si_bands = get_spectral_indices(s2_tile)\n",
    "                    if add_doy:\n",
    "                        date_parts = Path(fname).name.replace(\".tif\", \"\").split(\"_\")[-3:]\n",
    "                        date = \"_\".join(date_parts)\n",
    "                        s2_doy_band = date2doy(date, s2_tile.shape, norm_type, doy_mode, origin)\n",
    "\n",
    "                    s2_normal_tile = normalize(s2_tile, source, country, norm_type, clip)\n",
    "                        \n",
    "                    if bands == 10 and not (add_doy and add_si):\n",
    "                        data_array[:, :, :, idx] = s2_normal_tile\n",
    "                    elif bands == 11 and add_doy and not add_si:\n",
    "                        aug_array = np.concatenate([s2_normal_tile, s2_doy_band], axis=0)\n",
    "                        data_array[:, :, :, idx] = aug_array\n",
    "                    elif bands == 15 and add_si and not add_doy:\n",
    "                        aug_array = np.concatenate([s2_normal_tile, si_bands], axis=0)\n",
    "                        data_array[:, :, :, idx] = aug_array\n",
    "                    elif bands == 16 and (add_doy and add_si):\n",
    "                        aug_array = np.concatenate([s2_normal_tile, si_bands, s2_doy_band], axis=0)\n",
    "                        data_array[:, :, :, idx] = aug_array\n",
    "                        \n",
    "                with rasterio.open(mname) as msrc:\n",
    "                        mask_array[:, :, idx] = msrc.read()\n",
    "    \n",
    "            tmp_fn = Path(usable_cur_grid_files[0]).name.replace(\".tif\", \"\").split(\"_\")\n",
    "            fn = \"_\".join(tmp_fn[0:3])\n",
    "            out_fname = out_path / fn\n",
    "                \n",
    "            tmp_mn = Path(usable_cur_mask_files[0]).name.replace(\".tif\", \"\").split(\"_\")\n",
    "            mn = \"_\".join(tmp_mn[0:3])\n",
    "            out_mname = out_path / mn\n",
    "    \n",
    "            # store and save metadata\n",
    "            date_dict = {}\n",
    "            date_dict[\"dates\"] = dates\n",
    "    \n",
    "            with open(str(out_fname) + \".json\", \"w\") as fp:\n",
    "                json.dump(date_dict, fp)\n",
    "    \n",
    "            # save image stack as .npy\n",
    "            np.save(out_fname, data_array)\n",
    "            np.save(out_mname, mask_array)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            data_array = np.zeros((bands, img.shape[1], img.shape[2], len(usable_cur_grid_files)))\n",
    "            \n",
    "            for idx, fname in enumerate(usable_cur_grid_files):\n",
    "                \n",
    "                if verbose:\n",
    "                    print(\"idx: \", idx)\n",
    "                    print(\"fname: \", fname)\n",
    "        \n",
    "                with rasterio.open(fname) as src:\n",
    "                    s1_tile = src.read()\n",
    "                    s1_tile = s1_tile.astype(float)\n",
    "                    if not channel_first:\n",
    "                        s1_tile.transpose(2, 0, 1)\n",
    "                    \n",
    "                    s1_normal_tile = normalize(s1_tile, source, country, norm_type, clip)\n",
    "                    if add_doy:\n",
    "                        date_parts = Path(fname).name.replace(\".tif\", \"\").split(\"_\")[-3:]\n",
    "                        date = \"_\".join(date_parts)\n",
    "                        s1_doy_band = date2doy(date, s1_tile.shape, norm_type, doy_mode, origin)\n",
    "                        aug_array = np.concatenate([s1_normal_tile, s1_doy_band], axis=0)\n",
    "                        data_array[:, :, :, idx] = aug_array\n",
    "                    else:\n",
    "                        data_array[:, :, :, idx] = s1_normal_tile\n",
    "            \n",
    "            tmp_fn = Path(usable_cur_grid_files[0]).name.replace(\".tif\", \"\").split(\"_\")\n",
    "            fn = \"_\".join(tmp_fn[0:3])\n",
    "            out_fname = out_path / fn\n",
    "    \n",
    "            # store and save metadata\n",
    "            meta = {}\n",
    "            meta[\"dates\"] = dates\n",
    "    \n",
    "            with open(str(out_fname) + \".json\", \"w\") as fp:\n",
    "                json.dump(meta, fp)\n",
    "    \n",
    "            # save image stack as .npy\n",
    "            np.save(out_fname, data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#root_dir = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana\"\n",
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "source = \"Sentinel-1\"\n",
    "lbl_fldrname = \"Labels\"\n",
    "out_path = None \n",
    "norm_type = \"min/max\" \n",
    "clip = 1.5\n",
    "channel_first = True \n",
    "add_doy = True \n",
    "doy_mode = \"absolute\" \n",
    "origin = None \n",
    "add_si = False\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_temporal_cube(root_dir, country, source, lbl_fldrname, out_path, norm_type, \n",
    "                   clip, channel_first, add_doy, doy_mode, origin, add_si, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img = np.load(\"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana/Ghana/Sentinel-1/npy/source_s1_000334.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img[3,10:21, 10:21, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## step 5. Reclassify Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reclassify_lbl(root_dir, country, lbl_fldrname, categories, out_path=None, verbose=False):\n",
    "    #set_trace()\n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "    \n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for \\\n",
    "                  f in filenames if f.endswith(\".tif\")]\n",
    "    \n",
    "    complete_categories = {\"unknown\": 0, \"ground nut\": 1, \"maize\": 2, \"rice\": 3, \"soya bean\": 4, \"yam\": 5, \n",
    "                           \"intercrop\": 6, \"sorghum\": 7, \"okra\": 8, \"cassava\": 9, \"millet\": 10, \"tomato\": 11, \n",
    "                           \"cowpea\": 12, \"sweet potato\": 13, \"babala beans\": 14, \"salad vegetables\": 15, \n",
    "                           \"bra and ayoyo\": 16, \"watermelon\": 17, \"zabla\": 18, \"nili\": 19, \"kpalika\": 20, \n",
    "                           \"cotton\": 21, \"akata\": 22, \"nyenabe\": 23, \"pepper\": 24}\n",
    "    \n",
    "    for fn in lbl_fnames:\n",
    "        \n",
    "        with rasterio.open(fn) as src:\n",
    "            profile = src.profile\n",
    "            lbl_array = src.read()\n",
    "            \n",
    "            categories_ls = categories.keys()\n",
    "            categories_to_other = list(np.setdiff1d(list(complete_categories.keys()),list(categories.keys())))\n",
    "            aggregator_cat = list(np.setdiff1d(list(categories.keys()), list(complete_categories.keys())))\n",
    "            assert len(aggregator_cat) == 1, \"Your classification scheme contains invalid classes.\"\n",
    "            \n",
    "            if verbose:\n",
    "                lbl_id = str(fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "                print(\"---Grid: {} ---\".format(lbl_id))\n",
    "                print(\"List of crop categories: {} merged into the {} category\".format(categories_to_other, aggregator_cat))\n",
    "            \n",
    "            # initialize the canvas for reclassed layer\n",
    "            remapped_lbl = np.zeros_like((lbl_array), dtype=\"uint8\")\n",
    "            \n",
    "            for cat in categories_ls:\n",
    "                if cat == aggregator_cat[0]:\n",
    "                    for cat_to_other in categories_to_other:\n",
    "                        remapped_lbl[lbl_array == complete_categories[cat_to_other]] = categories[cat]\n",
    "                else:\n",
    "                    remapped_lbl[lbl_array == complete_categories[cat]] = categories[cat]\n",
    "            \n",
    "            if out_path is None:\n",
    "                out_path = lbl_dir.joinpath(\"reclass\")\n",
    "            Path(out_path).mkdir(parents=True, exist_ok=True)    \n",
    "            \n",
    "            \n",
    "            reclass_lbl_out_path = lbl_dir / \"reclass\"\n",
    "            Path(reclass_lbl_out_path).mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            profile.update(\n",
    "                dtype=rasterio.uint8\n",
    "            )\n",
    "                \n",
    "            with rasterio.open(Path(out_path) / fn.name, \"w\", **profile) as dst:\n",
    "                dst.write(remapped_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "lbl_fldrname = \"Labels\"\n",
    "categories = {\"unknown\": 0, \"maize\": 1, \"rice\": 2, \"other_crop\": 3}\n",
    "#categories = {\"unknown\": 0, \"ground nut\": 1, \"maize\": 2, \"rice\": 3, \"soya bean\": 4, \"other_crop\": 5}\n",
    "out_path = None\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "reclassify_lbl(root_dir, country, lbl_fldrname, categories, out_path, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fn1 = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana/Ghana/Labels/su_african_crops_ghana_labels_000001/labels_000001.tif\"\n",
    "fn2 = \"C:/My_documents/CropTypeData_Rustowicz/toy_Ghana/Ghana/Labels/reclass/labels_000001.tif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with rasterio.open(fn2) as src:\n",
    "    profile = src.profile\n",
    "    lbl_array = src.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lbl_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 6. Explore the temporal dimension and remove tiles with few dates that don't cover at least half (end of June) of the growing season (April-August) and those tiles that more than 75% of the crop field is covered with cloud in more than 70% of the temporal extent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 6.1 Check if tile IDs match between S1, S2 and Label folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def check_tile_ids_match(root_dir, country, lbl_fldrname):\n",
    "    \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
    "    lbl_fnames.sort()\n",
    "    \n",
    "    s1_src_path = Path(root_dir) / country / \"S1_npy\"\n",
    "    s1_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s1_src_path) for f in filenames if f.endswith(\".npy\")]\n",
    "    s1_fnames.sort()\n",
    "            \n",
    "    s2_src_path = Path(root_dir) / country / \"S2_npy\"\n",
    "    s2_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_src_path) for f in filenames if f.endswith(\".npy\") if \"source\" in f]\n",
    "    cmasks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_src_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    s2_fnames.sort()\n",
    "    cmasks.sort()\n",
    "    \n",
    "    for lbl_fn, s1_fn, s2_fn, cmask_fn in zip(lbl_fnames, s1_fnames, s2_fnames, cmasks):\n",
    "        \n",
    "        lbl_grid_id = str(lbl_fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        s1_grid_id = str(s1_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        s2_grid_id = str(s2_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        cmask_grid_id = str(s2_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        \n",
    "        print(\"label: \", lbl_grid_id)\n",
    "        print(\"S1: \", s1_grid_id)\n",
    "        print(\"S2: \", s2_grid_id)\n",
    "        print(\"cmask: \", cmask_grid_id)\n",
    "        assert lbl_grid_id == s1_grid_id == s2_grid_id == cmask_grid_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "lbl_fldrname = \"Labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "check_tile_ids_match(root_dir, country, lbl_fldrname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 6.2 Explore sequence length and available months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_seq_length_info(root_dir, country, source, seq_threshold, ext_file_path, verbose=False):\n",
    "    \n",
    "    src_path = Path(root_dir) / country / source\n",
    "    \n",
    "    files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".json\") if \"source\" in f]  \n",
    "    files.sort()\n",
    "    \n",
    "    out_dict = {}\n",
    "    flagged_grids = []\n",
    "    \n",
    "    out_dir = Path(ext_file_path)\n",
    "    name = \"{}_seq_length_report.txt\".format(str(source).split(\"_\")[0])\n",
    "    out_path = out_dir.joinpath(name)\n",
    "    \n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            seq_length = len(data[\"dates\"])\n",
    "            grid_id = str(file.name).split(\"_\")[-1].replace(\".json\", \"\")\n",
    "            out_dict[grid_id] = seq_length\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"--- Listing all ---\")\n",
    "                print(\"Grid_id: {}\".format(grid_id))\n",
    "                print(\"Temporal length: {}\".format(grid_id))\n",
    "                print(\"---\")\n",
    "            \n",
    "            if seq_length < seq_threshold:\n",
    "                \n",
    "                with open(out_path, \"a\") as external_file_1:\n",
    "                    print(\"Grid ID: {}, has {} timestamps.\".format(grid_id, seq_length), file=external_file_1)\n",
    "                \n",
    "                flagged_grids.append(grid_id)\n",
    "    \n",
    "    print(f\"Report is saved at: {out_path}\")            \n",
    "    return flagged_grids, out_dict\n",
    "\n",
    "#########################\n",
    "\n",
    "def get_available_months(root_dir, country, source, month_threshold, ext_file_path):\n",
    "     \n",
    "    src_path = Path(root_dir) / country / source\n",
    "    files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for\n",
    "             f in filenames if f.endswith(\".json\") if \"source\" in f]\n",
    "    \n",
    "    \n",
    "    all_ids_dict = {}\n",
    "    flagged_grids = []\n",
    "    \n",
    "    out_dir = Path(ext_file_path)\n",
    "    name = \"{}_avail_months_report.txt\".format(str(source).split(\"_\")[0])\n",
    "    out_path = out_dir.joinpath(name)\n",
    "    \n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            data = json.load(f)\n",
    "            grid_id = str(file.name).split(\"_\")[2].replace(\".json\", \"\")\n",
    "            months = [int(date.split(\"_\")[1]) for date in data[\"dates\"]]\n",
    "            dic_unique_count_months = dict(zip(months,[months.count(i) for i in months]))\n",
    "            available_months = len(dic_unique_count_months)\n",
    "            \n",
    "            all_ids_dict[grid_id] = available_months\n",
    "            \n",
    "            if available_months < month_threshold:\n",
    "    \n",
    "                with open(out_path, \"a\") as external_file_2:\n",
    "                    \n",
    "                    print(\"Grid ID: {}, has {} available months. Detail: {}\".format(grid_id, available_months, dic_unique_count_months), file=external_file_2)\n",
    "                \n",
    "                flagged_grids.append(grid_id)\n",
    "    \n",
    "    print(f\"Report is saved at: {out_path}\")\n",
    "    return flagged_grids, all_ids_dict          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "ext_file_path = \"D:/CropType/Ghana/Original_dataset/Ghana\"\n",
    "s1_seq_threshold = 15\n",
    "s2_seq_threshold = 20\n",
    "month_threshold = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s1_seq_flagged_grids, s1_seq_all_grids = get_seq_length_info(root_dir, country, source=\"S1_npy\", \n",
    "                                                             seq_threshold=s1_seq_threshold, ext_file_path=ext_file_path, verbose=False)\n",
    "s1_unique_tmp_lengths = Counter(list(s1_seq_all_grids.values()))\n",
    "s1_m_flagged_grids, s1_m_all_grids = get_available_months(root_dir, country, source=\"S1_npy\", \n",
    "                                                          month_threshold=month_threshold, ext_file_path=ext_file_path)\n",
    "\n",
    "print(\"--- Sentinel-1 ---\")\n",
    "print(\"Unique temporal lengths and their occurances (unique, count):\")\n",
    "print(sorted(s1_unique_tmp_lengths.items()))\n",
    "print(\"\")\n",
    "print(\"Flagged grids with shorter number of timestamps than {}:\".format(s1_seq_threshold))\n",
    "print(s1_seq_flagged_grids)\n",
    "print(\"total flagged tiles: \", len(s1_seq_flagged_grids))\n",
    "print(\"\")\n",
    "print(\"Flagged grids with data from a temporal extent of less than {} months:\".format(month_threshold))\n",
    "print(s1_m_flagged_grids)\n",
    "print(\"total flagged tiles: \", len(s1_m_flagged_grids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s2_seq_flagged_grids, s2_seq_all_grids = get_seq_length_info(root_dir, country, source=\"S2_npy\", \n",
    "                                                             seq_threshold=s2_seq_threshold, ext_file_path=ext_file_path, verbose=False)\n",
    "s2_unique_tmp_lengths = Counter(list(s2_seq_all_grids.values()))\n",
    "s2_m_flagged_grids, s2_m_all_grids = get_available_months(root_dir, country, source=\"S2_npy\", \n",
    "                                                          month_threshold=month_threshold, ext_file_path=ext_file_path)\n",
    "\n",
    "print(\"--- Sentinel-2 ---\")\n",
    "print(\"Unique temporal lengths and their occurances (unique, count):\")\n",
    "print(sorted(s2_unique_tmp_lengths.items()))\n",
    "print(\"\")\n",
    "print(\"Flagged grids with shorter number of timestamps than {}:\".format(s2_seq_threshold))\n",
    "print(s2_seq_flagged_grids)\n",
    "print(\"total flagged tiles: \", len(s2_seq_flagged_grids))\n",
    "print(\"\")\n",
    "print(\"Flagged grids with data from a temporal extent of less than {} months:\".format(month_threshold))\n",
    "print(s2_m_flagged_grids)\n",
    "print(\"total flagged tiles: \", len(s2_m_flagged_grids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 6.3 Explore the cloud coverage in the temporal extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reclass_cloudmask_stack(cloud_stack):\n",
    "    \"\"\" \n",
    "     Reclassify cloud mask values to a binary class of cloud and clear.\n",
    "     clear = 0 --> 0, clouds = 1  --> 1, shadows = 2 --> 1, haze = 3 --> 1\n",
    "     \n",
    "     output: numpy nd array with the same size as input.\n",
    "    \"\"\"\n",
    "    remapped_cloud_stack = np.zeros_like((cloud_stack))\n",
    "    remapped_cloud_stack[cloud_stack == 0] = 0\n",
    "    remapped_cloud_stack[cloud_stack == 1] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 2] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 3] = 1\n",
    "    \n",
    "    return remapped_cloud_stack\n",
    "\n",
    "############################################################\n",
    "\n",
    "def explore_cloudiness(root_dir, country, lbl_fldrname, ext_file_path, cloudiness_areal_threshold, cloudiness_numdates_threshold):\n",
    "    \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
    "    lbl_fnames.sort()\n",
    "    \n",
    "    src_path = Path(root_dir) / country / \"S2_npy\"\n",
    "    date_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".json\") if \"source\" in f]\n",
    "    cmask_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    date_fnames.sort()\n",
    "    cmask_fnames.sort()\n",
    "    \n",
    "    out_path = Path(ext_file_path).joinpath(\"S2_cloudiness_report.txt\")\n",
    "    if os.path.exists(out_path):\n",
    "        os.remove(out_path)\n",
    "    \n",
    "    flagged_grid_ids=[]\n",
    "    \n",
    "    for lbl_fn, date_fn, cmask_fn in zip(lbl_fnames, date_fnames, cmask_fnames):\n",
    "        lbl_grid_id = str(lbl_fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        date_grid_id = str(date_fn).split(\"_\")[-1].replace(\".json\", \"\")\n",
    "        cmask_grid_id = str(cmask_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        assert lbl_grid_id == date_grid_id == cmask_grid_id, \"problematic grid ids: lbl: {}, src: {}, cmask: {}\".format(\n",
    "            lbl_grid_id, date_grid_id, cmask_grid_id)\n",
    "        \n",
    "        with rasterio.open(lbl_fn, \"r\") as src:\n",
    "            if src.count != 1:\n",
    "                raise ValueError(\"Label must have only 1 band but {} bands were detected.\".format(src.count))\n",
    "            lbl_array = src.read(1)\n",
    "            crop_mask = np.where(lbl_array>0, 1, 0)\n",
    "            crop_area = np.sum(crop_mask)\n",
    "        \n",
    "        with open(date_fn, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            date_ls = data[\"dates\"]\n",
    "        \n",
    "        cmask_array = np.load(cmask_fn)\n",
    "        binary_cloud_mask = reclass_cloudmask_stack(cmask_array)\n",
    "        \n",
    "        temporal_length = len(date_ls)\n",
    "        \n",
    "        assert lbl_array.shape[0] == binary_cloud_mask.shape[0], \"problematic grid id: {}; lbl:{}, cmask:{}\".format(\n",
    "            lbl_grid_id, lbl_array.shape[0], binary_cloud_mask.shape[0])\n",
    "        assert lbl_array.shape[1] == binary_cloud_mask.shape[1], \"problematic grid id: {}; lbl:{}, cmask:{}\".format(\n",
    "            lbl_grid_id, lbl_array.shape[1], binary_cloud_mask.shape[1])\n",
    "        assert temporal_length == binary_cloud_mask.shape[2]\n",
    "        \n",
    "        detailed_dict ={}\n",
    "        counter = 0\n",
    "        \n",
    "        for i in range(temporal_length):\n",
    "            cloudy_crop_area = np.sum(crop_mask * binary_cloud_mask[:,:,i])\n",
    "            cloudiness_ratio = cloudy_crop_area / crop_area\n",
    "            \n",
    "            if cloudiness_ratio > cloudiness_areal_threshold:\n",
    "                detailed_dict[date_ls[i]] = cloudiness_ratio\n",
    "                counter += 1\n",
    "        \n",
    "        counter_ratio = counter / temporal_length \n",
    "        \n",
    "        if counter_ratio > cloudiness_numdates_threshold:       \n",
    "            with open(out_path, \"a\") as external_file:\n",
    "                print(f\"Grid ID: {lbl_grid_id}\", file=external_file)\n",
    "                \n",
    "                print(f\"With {counter} over areal_threshold cloudy days out of {temporal_length}\", file=external_file)\n",
    "                print(f\"details: {detailed_dict}\", file=external_file)\n",
    "                print(\"\", file=external_file)\n",
    "            \n",
    "            flagged_grid_ids.append(lbl_grid_id)\n",
    "    \n",
    "    print(f\"Report is saved at: {out_path}\")\n",
    "    return flagged_grid_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "lbl_fldrname = \"Labels\"\n",
    "ext_file_path = \"D:/CropType/Ghana/Original_dataset/Ghana\"\n",
    "cloudiness_areal_threshold = 0.75\n",
    "cloudiness_numdates_threshold = 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cloud_flags = explore_cloudiness(root_dir, country, lbl_fldrname, ext_file_path, cloudiness_areal_threshold, cloudiness_numdates_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(cloud_flags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Step 6.4 Move the flagged tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def MoveTiles(root_dir, country, lbl_fldrname, remove_tile_ls):\n",
    "    #set_trace()\n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname \n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
    "\n",
    "    s1_path = Path(root_dir) / country / \"S1_npy\"\n",
    "    s1_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s1_path) for f in filenames if f.endswith(\".npy\") if \"source\" in f]\n",
    "    s1_meta_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s1_path) for f in filenames if f.endswith(\".json\")]\n",
    "    s2_path = Path(root_dir) / country / \"S2_npy\"\n",
    "    s2_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_path) for f in filenames if f.endswith(\".npy\") if \"source\" in f]\n",
    "    s2_meta_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_path) for f in filenames if f.endswith(\".json\")]\n",
    "    cloud_masks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    \n",
    "    assert len(s1_fnames) == len(s2_fnames) == len(lbl_fnames) == len(s1_meta_fnames) == len(s1_meta_fnames) == len(cloud_masks)\n",
    "    lbl_fnames.sort()\n",
    "    s1_fnames.sort()\n",
    "    s1_meta_fnames.sort()\n",
    "    s2_fnames.sort()\n",
    "    s2_meta_fnames.sort()\n",
    "    cloud_masks.sort()\n",
    "\n",
    "    s1_out_path = s1_path / \"S1_moved\"\n",
    "    s2_out_path = s2_path / \"S2_moved\"\n",
    "    lbl_out_path = lbl_dir / \"lbl_moved\"\n",
    "    \n",
    "    dirs = [s1_out_path, s2_out_path, lbl_out_path]\n",
    "    \n",
    "    for p in dirs:\n",
    "        if not os.path.exists(p):\n",
    "            os.makedirs(p)\n",
    "\n",
    "    for s1_fn, s1_meta_fn, s2_fn, s2_meta_fn, cmask_fn, lbl_fn in zip(s1_fnames, s1_meta_fnames, s2_fnames, s2_meta_fnames, cloud_masks, lbl_fnames):\n",
    "        \n",
    "        s1_grid_id = str(s1_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        s2_grid_id = str(s2_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        lbl_grid_id = str(lbl_fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        assert s1_grid_id == s2_grid_id == lbl_grid_id, \"Grid Id mis-match between Sentinel-1 & 2 chips.\"\n",
    "        \n",
    "        if s1_grid_id in remove_tile_ls:\n",
    "            shutil.move(str(s1_fn), str(s1_out_path))\n",
    "            shutil.move(str(s1_meta_fn), str(s1_out_path))\n",
    "            shutil.move(str(s2_fn), str(s2_out_path))\n",
    "            shutil.move(str(s2_meta_fn), str(s2_out_path))\n",
    "            shutil.move(str(cmask_fn), str(s2_out_path))\n",
    "            shutil.move(str(lbl_fn), str(lbl_out_path))\n",
    "        \n",
    "    print(\"End of process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset\"\n",
    "country = \"Ghana\"\n",
    "lbl_fldrname = \"Labels\"\n",
    "remove_tile_ls = cloud_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "MoveTiles(root_dir, country, lbl_fldrname, remove_tile_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 7. Add Non-crop class to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_fid_coord(num_mesh_cells, img_extent, start_indexing=\"upper left\", index_pos=\"center\"):\n",
    "    \n",
    "    r\"\"\"Generated Mesh grid is matching based on the 'num_mesh_cells' and 'img_extent'.\n",
    "    Params:\n",
    "    num_mesh_cells (int) -- Number of mesh cells.\n",
    "    img_extent (int) -- Either number of row or colums of the square image patch.\n",
    "    start_indexing (str): point of origin to index mesh grids. \n",
    "    index_pos (str) -- position of the identifier for each mesh cell.  \n",
    "    \n",
    "    Note 1: 'num_mesh_cells' and 'img_extent' must be divisible.\n",
    "    Note 2: ArcGIS mesh-grids start indexing for FID of point labels in the center of cells from lower left corner\n",
    "          of the img as opposed to numpy array indexing that starts from upper left.\n",
    "    Note 3: If you want fids to sample a numpy array, choose \"upper left\" for 'start_indexing',\n",
    "          Otherwise if you wish to simulate FID naming of ArcGIS mesh grid, then choose: \"lower left\".\"\"\"\n",
    "    \n",
    "    \n",
    "    assert index_pos in [\"upper left\", \"center\"], \"Invalid index type.\"\n",
    "    assert start_indexing in [\"upper left\", \"lower left\"], \"Invalid index type.\"\n",
    "    assert (num_mesh_cells % img_extent) == 0, \"'num_mesh_cells' must be divisible by 'img_extent'.\"\n",
    "    \n",
    "    fids = np.arange(num_mesh_cells)\n",
    "    h = w = img_extent\n",
    "    grid_size = len(fids) // img_extent\n",
    "    \n",
    "    if index_pos == \"center\":\n",
    "        x_ls = range(grid_size//2, h - (grid_size//2)+1, grid_size)\n",
    "        if start_indexing == \"upper left\":\n",
    "            x_ls = x_ls[::-1]\n",
    "        y_ls = range(grid_size//2, w - (grid_size//2)+1, grid_size)\n",
    "    else:\n",
    "        x_ls = range(0, h, grid_size)\n",
    "        if start_indexing == \"upper left\":\n",
    "            x_ls = x_ls[::-1]\n",
    "        y_ls = range(0, w, grid_size)\n",
    "    \n",
    "    index = list(itertools.product(x_ls, y_ls))\n",
    "    fid_coord_dict = dict((str(fid), idx) for (fid, idx) in zip(fids, index))\n",
    "    \n",
    "    return fid_coord_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_mesh_cells = 256\n",
    "img_extent=64\n",
    "start_indexing = \"upper left\" \n",
    "index_pos = \"center\"\n",
    "mesh_grid_dict = get_fid_coord(num_mesh_cells, img_extent, start_indexing, index_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def add_non_crop_to_lbl(lbl_dir, csv_path, mesh_grid_dict, out_dir=None, verbose=False):\n",
    "    \n",
    "    tile_fid_ls = pd.read_csv(csv_path, index_col=\"ID\")\n",
    "    tile_ids_to_reclass = list(tile_fid_ls.index)\n",
    "    \n",
    "    lbl_dir = Path(lbl_dir)\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for \\\n",
    "                  f in filenames if f.endswith(\".tif\")]\n",
    "    \n",
    "    if out_dir is None:\n",
    "        out_dir = lbl_dir.joinpath(\"reclass\")\n",
    "    Path(out_dir).mkdir(parents=True, exist_ok=True)    \n",
    "    \n",
    "    for fn in lbl_fnames:\n",
    "        lbl_id = str(fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        \n",
    "        if int(lbl_id) in tile_ids_to_reclass:\n",
    "            \n",
    "            sample_fid_ls = list(tile_fid_ls.loc[int(lbl_id)])[0].split(\",\")\n",
    "            \n",
    "            num_samples = len(sample_fid_ls) * 16\n",
    "            if verbose:\n",
    "                print(f\"{num_samples} non-crop pixels are added to the tile: {lbl_id}\")\n",
    "\n",
    "            with rasterio.open(fn) as src:\n",
    "                profile = src.profile\n",
    "                lbl_array = src.read()\n",
    "                \n",
    "                profile.update(\n",
    "                    dtype=rasterio.uint8\n",
    "                )\n",
    "                \n",
    "                remapped_lbl = np.zeros_like((lbl_array), dtype=\"uint8\")\n",
    "                remapped_lbl[lbl_array == 1] = 1\n",
    "                remapped_lbl[lbl_array == 2] = 2\n",
    "                remapped_lbl[lbl_array == 3] = 3\n",
    "                \n",
    "                for fid in sample_fid_ls:\n",
    "                    fid_coord = mesh_grid_dict[fid.strip()]\n",
    "                    row = fid_coord[0]\n",
    "                    col = fid_coord[1]\n",
    "                    \n",
    "                    if verbose:\n",
    "                        if lbl_array[:, row-2:row+2, col-2:col+2].any() > 0:\n",
    "                            print(f\"Bad FID sample: {fid}\")\n",
    "                    \n",
    "                    if lbl_array[:, row-2:row+2, col-2:col+2].all() == 0:\n",
    "                        remapped_lbl[:, row-2:row+2, col-2:col+2] = 4\n",
    "                    \n",
    "            with rasterio.open(Path(out_dir) / fn.name, \"w\", **profile) as dst:\n",
    "                dst.write(remapped_lbl)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"No change to the tile: {lbl_id}\")\n",
    "            shutil.copy(fn, out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lbl_dir = \"C:/My_documents/CropTypeData_Rustowicz/Ghana/Labels\"\n",
    "csv_path = \"C:/My_documents/CropTypeData_Rustowicz/Ghana/usable_training_data.csv\"\n",
    "out_dir = None\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "add_non_crop_to_lbl(lbl_dir, csv_path, mesh_grid_dict, out_dir, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## step 8. Summarize statistics of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\"\"\"\n",
    "\n",
    "def summarize_lbl(lbl_dir, out_filename, category = None):\n",
    "    \n",
    "    lbl_dir = Path(lbl_dir)\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for \\\n",
    "                  f in filenames if f.endswith(\".tif\")]\n",
    "    \n",
    "    lbl_ids = [str(f).split(\"_\")[-1].replace(\".tif\", \"\") for f in lbl_fnames]\n",
    "    \n",
    "    if category:\n",
    "        category_dict = category\n",
    "    else:\n",
    "        category_dict = {\"unknown\": 0, \"ground nut\": 1, \"maize\": 2, \"rice\": 3, \"soya bean\": 4, \"yam\": 5, \n",
    "                         \"intercrop\": 6, \"sorghum\": 7, \"okra\": 8, \"cassava\": 9, \"millet\": 10, \"tomato\": 11, \n",
    "                         \"cowpea\": 12, \"sweet potato\": 13, \"babala beans\": 14, \"salad vegetables\": 15, \n",
    "                         \"bra and ayoyo\": 16, \"watermelon\": 17, \"zabla\": 18, \"nili\": 19, \"kpalika\": 20, \n",
    "                         \"cotton\": 21, \"akata\": 22, \"nyenabe\": 23, \"pepper\": 24}\n",
    "    \n",
    "    key_list = list(category_dict.keys())\n",
    "    val_list = list(category_dict.values())\n",
    "    \n",
    "    df = pd.DataFrame(columns = key_list, index = lbl_ids)\n",
    "    \n",
    "    for fn in lbl_fnames:\n",
    "        lbl_id = fn.name.split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        \n",
    "        with rasterio.open(fn) as src:\n",
    "            lbl_array = src.read()\n",
    "            categories, counts = np.unique(lbl_array, return_counts=True)\n",
    "            for a,b in zip(list(categories), list(counts)):\n",
    "                if lbl_id in list(df.index):\n",
    "                    if key_list[val_list.index(a)] in list(df.columns):\n",
    "                        df.loc[lbl_id, key_list[val_list.index(a)]] = b\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(lbl_dir / out_filename, index_label='Grid-ID')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lbl_dir = \"D:/CropType/Ghana/Labels/validation\"\n",
    "out_filename = \"report.csv\"\n",
    "category = {\"unknown\": 0, \"maize\": 1, \"rice\": 2, \"other_crop\": 3}\n",
    "#category = {\"unknown\": 0, \"maize\": 1, \"rice\": 2, \"other_crop\": 3, \"non_crop\": 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "report = summarize_lbl(lbl_dir, out_filename, category)\n",
    "report.sum(axis = 0, skipna = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 9. Split the dataset into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "\n",
    "def create_grid_splits(root_dir, country, sources, lbl_fldrname, csv_fn, split_threshold):\n",
    "    \"\"\"\n",
    "    Splitting the dataset into train and test datasets.\n",
    "    \n",
    "    root_dir (str) -- path to the main directory which data resides. Example: \"C:/My_documents/Data\".\n",
    "    country (str) -- This is based on the organization of dataset that images and \n",
    "                     labels reside inside a country folder.\n",
    "    sources (list) -- folder name of the image resource. example: [\"Sentinel-1\", \"Sentinel-2\"]\n",
    "    lbl_fldrname (str) -- Name of the folder containing annotated grids.\n",
    "    csv_fn (str) -- Name of the csv file summerizing the content of each grid.\n",
    "    split_threshold (float) -- scalar value as a threshold to decide how many of the grids will be\n",
    "                               in the training folder. Default is 0.8.\n",
    "    \"\"\"\n",
    "    \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for \\\n",
    "                  f in filenames if f.endswith(\".tif\")]\n",
    "    lbl_ids = [str(f).split(\"_\")[-1].replace(\".tif\", \"\") for f in lbl_fnames]\n",
    "    \n",
    "    report = pd.read_csv(lbl_dir / csv_fn, index_col=\"Grid-ID\")\n",
    "    assert(report.shape[0] == len(lbl_ids))\n",
    "    \n",
    "    num_train_girds = math.ceil(split_threshold * len(lbl_ids))\n",
    "    categories = list(report.columns)\n",
    "    \n",
    "    # make a dictionary with the keys as category names and values as list of grid-ids containing\n",
    "    # those categories.\n",
    "    cat_grid = {}\n",
    "    for category in categories:\n",
    "        if category != \"unknown\":\n",
    "            cat_grid[category] = list(report[report[category] > 0].index)\n",
    "    \n",
    "    # Choose the category with the least amount of tiles as the initial category and add the sampled ids\n",
    "    # in the list of training grids.\n",
    "    initial_category = min(cat_grid, key=lambda cat: len(cat_grid[cat]))\n",
    "    num_initial_tiles = math.ceil(len(cat_grid[initial_category]) * split_threshold)\n",
    "    training_grids = random.sample(cat_grid[initial_category], num_initial_tiles)\n",
    "    \n",
    "    # for each category find the similar grid-ids that are already in the training list. Recalculate the number of\n",
    "    # samples that need to be taken. Sample unique grid-ids for the category and add it to the list of training grids.\n",
    "    for category in categories:\n",
    "        if category not in [\"unknown\", initial_category]:\n",
    "            similar_grids = set(cat_grid[category]).intersection(training_grids)\n",
    "            num_samples_to_take = math.ceil(len(cat_grid[category]) * split_threshold) - len(similar_grids)\n",
    "            allowable_grids = list(np.setdiff1d(cat_grid[category], training_grids))\n",
    "            grid_ids = random.sample(allowable_grids, num_samples_to_take)\n",
    "            training_grids.extend(grid_ids)\n",
    "    \n",
    "    # make sure that training folder contains correct number of grids as decided by the split threshold.\n",
    "    #To do that we add or drop grids from the category with the max number of grids.\n",
    "    if len(training_grids) < num_train_girds:\n",
    "        num_extra_samples = num_train_girds - len(training_grids)\n",
    "        biggest_category = max(cat_grid, key=lambda cat: len(cat_grid[cat]))\n",
    "        allowable_other_grids = list(np.setdiff1d(cat_grid[biggest_category], training_grids))\n",
    "        extra_other_grid_ids = random.sample(allowable_other_grids, num_extra_samples)\n",
    "        training_grids.extend(extra_other_grid_ids)\n",
    "    else:\n",
    "        num_samples_to_drop = len(training_grids) - num_train_girds\n",
    "        biggest_category = max(cat_grid, key=lambda cat: len(cat_grid[cat]))\n",
    "        allowable_other_grids = set(cat_grid[biggest_category]).intersection(training_grids)\n",
    "        droppable_other_grid_ids = random.sample(allowable_other_grids, num_samples_to_drop)\n",
    "        for item in training_grids:\n",
    "            if item in droppable_other_grid_ids:\n",
    "                training_grids.remove(item)\n",
    "    \n",
    "    # add preceding zeros to the list of training grids\n",
    "    training_grids = [str(item).zfill(6) for item in training_grids]\n",
    "    val_grids = list(np.setdiff1d(lbl_ids, training_grids))\n",
    "    training_grids.sort()\n",
    "    val_grids.sort()\n",
    "    \n",
    "    # Create proper folders for the splitted dataset\n",
    "    lbl_train_out_path = lbl_dir / \"train\"\n",
    "    lbl_val_out_path = lbl_dir / \"validation\"\n",
    "    Path(lbl_train_out_path).mkdir(parents=True, exist_ok=True)\n",
    "    Path(lbl_val_out_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Copy labels into train, validate subfolders.\n",
    "    for id, fn in zip(lbl_ids, lbl_fnames):\n",
    "        \n",
    "        if (id in training_grids) and (id in fn.name):\n",
    "            #shutil.copy(fn, lbl_train_out_path)\n",
    "            shutil.move(str(fn), str(lbl_train_out_path))\n",
    "        \n",
    "        if (id in val_grids) and (id in fn.name):\n",
    "            #shutil.copy(fn, lbl_val_out_path)\n",
    "            shutil.move(str(fn), str(lbl_val_out_path))\n",
    "    \n",
    "    # Copy the img dataset based on the grid-ID to equivalent subfolders.\n",
    "    for source in sources:        \n",
    "        \n",
    "        src_dir = Path(root_dir) / country / source\n",
    "        src_train_out_path = src_dir / \"train\"\n",
    "        src_val_out_path = src_dir / \"validation\"\n",
    "        \n",
    "        Path(src_train_out_path).mkdir(parents=True, exist_ok=True)\n",
    "        Path(src_val_out_path).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        src_files = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_dir) for \\\n",
    "                     f in filenames]\n",
    "        \n",
    "        for id in lbl_ids:\n",
    "            for fn in src_files:\n",
    "                if (id in training_grids) and (id in fn.name):\n",
    "                    #shutil.copy(fn, src_train_out_path)\n",
    "                    shutil.move(str(fn), str(src_train_out_path))\n",
    "            \n",
    "                if (id in val_grids) and (id in fn.name):\n",
    "                    #shutil.copy(fn, src_val_out_path)\n",
    "                    shutil.move(str(fn), str(src_val_out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType\"\n",
    "country = \"Ghana\"\n",
    "sources = [\"S1_npy\", \"S2_npy\"]\n",
    "lbl_fldrname = \"Labels\"\n",
    "csv_fn = \"report.csv\"\n",
    "split_threshold = 0.61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "create_grid_splits(root_dir, country, sources, lbl_fldrname, csv_fn, split_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Step 10. Make pixel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reclass_cloudmask_stack(cloud_stack):\n",
    "    \"\"\" \n",
    "     Reclassify cloud mask values to a binary class of cloud and clear.\n",
    "     clear = 0 --> 0, clouds = 1  --> 1, shadows = 2 --> 1, haze = 3 --> 1\n",
    "     \n",
    "     output: numpy nd array with the same size as input.\n",
    "    \"\"\"\n",
    "    remapped_cloud_stack = np.zeros_like((cloud_stack))\n",
    "    remapped_cloud_stack[cloud_stack == 0] = 0\n",
    "    remapped_cloud_stack[cloud_stack == 1] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 2] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 3] = 1\n",
    "    \n",
    "    return remapped_cloud_stack\n",
    "\n",
    "############################################################\n",
    "\n",
    "def load_data(dataPath, isLabel = False):\n",
    "    \"\"\"Load the dataset.\n",
    "    Args:\n",
    "        dataPath (str) -- Path to either the image or label raster.\n",
    "        isLabel (binary) -- decide wether the input dataset is label. Default is False.\n",
    "    \n",
    "    Returns:\n",
    "        loaded data as numpy ndarray. \n",
    "    \"\"\"\n",
    "    \n",
    "    if isLabel:\n",
    "        \n",
    "        with rasterio.open(dataPath, \"r\") as src:\n",
    "            \n",
    "            if src.count != 1:\n",
    "                raise ValueError(\"Label must have only 1 band but {} bands were detected.\".format(src.count))\n",
    "            img = src.read(1)\n",
    "    \n",
    "    else:\n",
    "        img = np.load(dataPath)\n",
    "    \n",
    "    return img\n",
    "\n",
    "############################################################\n",
    "\n",
    "\n",
    "def Make_pixel_dataset(root_dir, country, sources, lbl_fldrname, categories, usage, verbose):\n",
    "    \n",
    "    lbl_dir = Path(root_dir) / country / lbl_fldrname / usage\n",
    "    lbl_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(lbl_dir) for f in filenames if f.endswith(\".tif\")]\n",
    "    lbl_fnames.sort()\n",
    "\n",
    "    s1_src_path = Path(root_dir) / country / \"Sentinel-1\" / usage\n",
    "    s1_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s1_src_path) for f in filenames if f.endswith(\".npy\")]\n",
    "    s1_fnames.sort()\n",
    "            \n",
    "    s2_src_path = Path(root_dir) / country / \"Sentinel-2\" / usage\n",
    "    s2_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_src_path) for f in filenames if f.endswith(\".npy\") if \"source\" in f]\n",
    "    cmasks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(s2_src_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    s2_fnames.sort()\n",
    "    cmasks.sort()\n",
    "    \n",
    "    category_list = list(categories.keys())\n",
    "    inv_category_map = {v: k for k, v in categories.items()}\n",
    "    \n",
    "    for cat in category_list:\n",
    "        s1_out_dir = s1_src_path / cat\n",
    "        s2_out_dir = s2_src_path / cat\n",
    "        Path(s1_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(s2_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    for lbl_fn, s1_fn, s2_fn, cmask_fn in tqdm.tqdm(zip(lbl_fnames, s1_fnames, s2_fnames, cmasks), total = len(lbl_fnames)):\n",
    "        \n",
    "        lbl_grid_id = str(lbl_fn).split(\"_\")[-1].replace(\".tif\", \"\")\n",
    "        s1_grid_id = str(s1_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        s2_grid_id = str(s2_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        cmask_grid_id = str(cmask_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        assert lbl_grid_id == s1_grid_id == s2_grid_id == cmask_grid_id, \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        \n",
    "        lbl_array = load_data(lbl_fn, isLabel = True)\n",
    "        s1_array = load_data(s1_fn, isLabel = False)\n",
    "        s2_array = load_data(s2_fn, isLabel = False)\n",
    "        s2_array[s2_array == +inf] = 0\n",
    "        s2_array[s2_array == -inf] = 0\n",
    "        cmask_array = load_data(cmask_fn, isLabel = False)\n",
    "        binary_cloud_array = reclass_cloudmask_stack(cmask_array)\n",
    "        \n",
    "        assert lbl_array.shape[0] == s1_array.shape[1] == s2_array.shape[1] == cmask_array.shape[0], \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        assert lbl_array.shape[1] == s1_array.shape[2] == s2_array.shape[2] == cmask_array.shape[1], \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        \n",
    "        unique_vals, unique_counts = np.unique(lbl_array, return_counts=True)\n",
    "        cloudy_days = np.sum(binary_cloud_array, axis=2)\n",
    "        cloudy_days = cloudy_days / cmask_array.shape[0]\n",
    "        \n",
    "        for val, count in zip(unique_vals, unique_counts):\n",
    "            mask = lbl_array == [val]\n",
    "            crop_indices = np.where(mask)\n",
    "            crop_coordinates = list(zip(crop_indices[0], crop_indices[1]))\n",
    "            \n",
    "            if val == 0:\n",
    "                num_samples = min(60, np.sum(mask))\n",
    "                crop_coordinates = random.sample(crop_coordinates, num_samples)\n",
    "\n",
    "            cr_ls = [cloudy_days[coord[0], coord[1]] for coord in crop_coordinates]\n",
    "            df = pd.DataFrame(zip(crop_coordinates, cr_ls), columns=['Coordinates','Cloudiness'])\n",
    "            df = df.sort_values(\"Cloudiness\")\n",
    "            ranked_crop_coordinates = list(df.Coordinates)\n",
    "                \n",
    "            if verbose:\n",
    "                print(\"Grid ID: {}, number of {} samples: {}\".format(lbl_grid_id, inv_category_map[val], len(crop_coordinates)))\n",
    "            \n",
    "            for index, coord in enumerate(ranked_crop_coordinates, start=1):\n",
    "                lbl_val = lbl_array[coord[0], coord[1]]\n",
    "                #lbl_out_path = lbl_dir / inv_category_map[val]\n",
    "                #lbl_out_fname = \"lbl_\"+lbl_grid_id+\"_sample_\"+str(index)\n",
    "                #np.save(lbl_out_path / lbl_out_fname, lbl_val)\n",
    "                \n",
    "                s1_val = s1_array[:,coord[0], coord[1],:]\n",
    "                s1_out_path = s1_src_path / inv_category_map[val]\n",
    "                s1_out_fname = \"s1_\"+s1_grid_id+\"_sample_\"+str(index)+\"_lbl_\"+str(lbl_val)\n",
    "                np.save(s1_out_path / s1_out_fname, s1_val)\n",
    "                \n",
    "                s2_val = s2_array[:,coord[0], coord[1],:]\n",
    "                s2_out_path = s2_src_path / inv_category_map[val]\n",
    "                s2_out_fname = \"s2_\"+s2_grid_id+\"_sample_\"+str(index)+\"_lbl_\"+str(lbl_val)\n",
    "                np.save(s2_out_path / s2_out_fname, s2_val)\n",
    "                    \n",
    "                #cmask_val = cmask_array[coord[0], coord[1],:]\n",
    "                #cmask_out_fname = \"cmask_\"+cmask_grid_id+\"_sample_\"+str(index)\n",
    "                #np.save(s2_out_path / cmask_out_fname, cmask_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType\"\n",
    "#root_dir = \"C:/My_documents/CropTypeData_Rustowicz/CropType\"\n",
    "country = \"Ghana\"\n",
    "sources = [\"Sentinel-1\", \"Sentinel-2\"]\n",
    "lbl_fldrname = \"Labels\"\n",
    "usage = \"validation\"\n",
    "#categories = {\"unknown\": 0, \"maize\": 1, \"rice\": 2, \"other_crop\": 3, \"non_crop\": 4}\n",
    "categories = {\"unknown\": 0, \"maize\": 1, \"rice\": 2, \"other_crop\": 3}\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Make_pixel_dataset(root_dir, country, sources, lbl_fldrname, categories, usage, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Make temporal cloud mask for non-crop sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reclass_cloudmask_stack(cloud_stack):\n",
    "    \"\"\" \n",
    "     Reclassify cloud mask values to a binary class of cloud and clear.\n",
    "     clear = 0 --> 0, clouds = 1  --> 1, shadows = 2 --> 1, haze = 3 --> 1\n",
    "     \n",
    "     output: numpy nd array with the same size as input.\n",
    "    \"\"\"\n",
    "    remapped_cloud_stack = np.zeros_like((cloud_stack))\n",
    "    remapped_cloud_stack[cloud_stack == 0] = 0\n",
    "    remapped_cloud_stack[cloud_stack == 1] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 2] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 3] = 1\n",
    "    \n",
    "    return remapped_cloud_stack\n",
    "\n",
    "\n",
    "def Make_temporal_cloud_mask(root_dir, source, cday_threshold):\n",
    "    \n",
    "    src_path = Path(root_dir) / source\n",
    "    src_fnames = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".npy\") if \"source\" in f]\n",
    "    cmasks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    src_fnames.sort()\n",
    "    cmasks.sort()\n",
    "        \n",
    "    outdir = Path(root_dir) / \"temporal_cloud_mask\"\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for src_fn, cmask_fn in tqdm.tqdm(zip(src_fnames, cmasks), total = len(src_fnames)):\n",
    "        \n",
    "        src_grid_id = str(src_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        cmask_grid_id = str(cmask_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        assert src_grid_id == cmask_grid_id, \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        \n",
    "        src_array = np.load(src_fn)\n",
    "        cmask_array = np.load(cmask_fn)\n",
    "        binary_cloud_array = reclass_cloudmask_stack(cmask_array)\n",
    "        \n",
    "        assert src_array.shape[1] == cmask_array.shape[1], \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        assert src_array.shape[2] == cmask_array.shape[1], \"problematic grid id: {}\".format(lbl_grid_id)\n",
    "        \n",
    "        cloudy_days = np.sum(binary_cloud_array, axis=(0,3))\n",
    "        cloudy_days = cloudy_days / cmask_array.shape[0]\n",
    "        temporal_cloud_mask = np.where(cloudy_days < cday_threshold, 1, 0)\n",
    "        \n",
    "        out_profile = {\n",
    "            'driver': 'GTiff', 'dtype': 'int32', 'nodata': None, 'width': 64, 'height': 64, 'count': 1, 'crs': None,\n",
    "            'transform': rasterio.Affine(1.0, 0.0, 0.0,0.0, 1.0, 0.0), 'tiled': False, 'interleave': 'band'\n",
    "                       }\n",
    "        \n",
    "        out_name = \"temporal_cloud_mask_22days_{}.tif\".format(src_grid_id)\n",
    "        with rasterio.open(Path(outdir) / out_name, \"w\", **out_profile) as dst:\n",
    "            dst.write(np.expand_dims(temporal_cloud_mask, 0))\n",
    "        \n",
    "    print(\"SEOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reclass_cloudmask_stack(cloud_stack):\n",
    "    \"\"\" \n",
    "     Reclassify cloud mask values to a binary class of cloud and clear.\n",
    "     clear = 0 --> 0, clouds = 1  --> 1, shadows = 2 --> 1, haze = 3 --> 1\n",
    "     \n",
    "     output: numpy nd array with the same size as input.\n",
    "    \"\"\"\n",
    "    remapped_cloud_stack = np.zeros_like((cloud_stack))\n",
    "    remapped_cloud_stack[cloud_stack == 0] = 0\n",
    "    remapped_cloud_stack[cloud_stack == 1] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 2] = 1\n",
    "    remapped_cloud_stack[cloud_stack == 3] = 1\n",
    "    \n",
    "    return remapped_cloud_stack\n",
    "\n",
    "\n",
    "def Make_temporal_cloud_mask(root_dir, source, cday_threshold):\n",
    "    \n",
    "    src_path = Path(root_dir) / source\n",
    "    cmasks = [Path(dirpath) / f for (dirpath, dirnames, filenames) in os.walk(src_path) for f in filenames if f.endswith(\".npy\") if \"cloudmask\" in f]\n",
    "    cmasks.sort()\n",
    "        \n",
    "    outdir = Path(root_dir) / \"temporal_cloud_mask\"\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for cmask_fn in tqdm.tqdm(cmasks):\n",
    "        \n",
    "        cmask_grid_id = str(cmask_fn).split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "        cmask_array = np.load(cmask_fn)\n",
    "        binary_cloud_array = reclass_cloudmask_stack(cmask_array)\n",
    "        \n",
    "        cloudy_days = np.sum(binary_cloud_array, axis=2)\n",
    "        cloudy_days = cloudy_days / cmask_array.shape[0]\n",
    "        temporal_cloud_mask = np.where(cloudy_days < cday_threshold, 1, 0)\n",
    "        \n",
    "        out_profile = {\n",
    "            'driver': 'GTiff', 'dtype': 'int32', 'nodata': None, 'width': 64, 'height': 64, 'count': 1, 'crs': None,\n",
    "            'transform': rasterio.Affine(1.0, 0.0, 0.0,0.0, 1.0, 0.0), 'tiled': False, 'interleave': 'band'\n",
    "                       }\n",
    "        \n",
    "        out_name = \"temporal_cloud_mask_0.5_{}.tif\".format(cmask_grid_id)\n",
    "        with rasterio.open(Path(outdir) / out_name, \"w\", **out_profile) as dst:\n",
    "            dst.write(np.expand_dims(temporal_cloud_mask, 0))\n",
    "        \n",
    "    print(\"SEOO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "root_dir = \"D:/CropType/Ghana/Original_dataset/Ghana\"\n",
    "source = \"S2_npy\"\n",
    "cday_threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Make_temporal_cloud_mask(root_dir, source, cday_threshold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.856px",
    "left": "1166px",
    "right": "20px",
    "top": "119px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
